{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_features.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGTgZWhUKd9W"
      },
      "source": [
        "!pip install nltk spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOU6f7YuKn-2"
      },
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "# !python -m spacy link xx_ent_wiki_sm xx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrQcNXSSK3xl"
      },
      "source": [
        "!pip install langid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xka6XOgKkyr"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSfIsfoGKvZM"
      },
      "source": [
        "nlp = spacy.load('xx', disable=['ner', 'parser'])\n",
        "\n",
        "def cleaning(doc):\n",
        "    # Lemmatizes and removes stopwords\n",
        "    # doc needs to be a spacy Doc object\n",
        "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
        "    # if a sentence is only one or two words long,\n",
        "    # the benefit for the training is very small\n",
        "    if len(txt) > 2:\n",
        "        return ' '.join(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIHSuJohK1aP"
      },
      "source": [
        "article_info.title = article_info.title.str.lower()\n",
        "article_info.paperAbstract = article_info.paperAbstract.str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3KdLPBqK6EY"
      },
      "source": [
        "import langid # language identification (i.e. what language is this?)\n",
        "from nltk.classify.textcat import TextCat # language identification from NLTK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Z5EZy8K_mm"
      },
      "source": [
        "# taken from https://www.kaggle.com/rtatman/analyzing-multilingual-data\n",
        "# lang_ids = article_info['title'].apply(langid.classify)\n",
        "langs = lang_ids.apply(lambda tuple: tuple[0])\n",
        "\n",
        "# how many unique language labels were applied?\n",
        "print(\"Number of tagged languages (estimated):\")\n",
        "print(len(langs.unique()))\n",
        "\n",
        "# percent of the total dataset in English\n",
        "print(\"Percent of data in English (estimated):\")\n",
        "print((sum(langs==\"en\")/len(langs))*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzjA19AFLCfs"
      },
      "source": [
        "# Number of tagged languages (estimated):\n",
        "# 62\n",
        "# Percent of data in English (estimated):\n",
        "# 96.87297408520259"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff4xjxS_LFXS"
      },
      "source": [
        "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in article_info['title'] if article_info['abstract'] is not np.nan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wug4n7uLHWo"
      },
      "source": [
        "article_info.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gnypNYyLLMC"
      },
      "source": [
        "article_info.fieldsOfStudy = article_info.fieldsOfStudy.apply(lambda x: np.nan if x[0] is np.nan else x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpIoNtjFLREx"
      },
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "X = mlb.fit_transform(article_info.fieldsOfStudy)\n",
        "\n",
        "article_info = article_info.join(pd.DataFrame(X, columns=mlb.classes_, index=article_info.index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGIfxyMlLeHW"
      },
      "source": [
        "article_info.drop('fieldsOfStudy', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnNfwiFGLYbe"
      },
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "X = mlb.fit_transform(article_info.sources)\n",
        "\n",
        "article_info = article_info.join(pd.DataFrame(X, columns=mlb.classes_, index=article_info.index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C08Z3DoLcYk"
      },
      "source": [
        "article_info.drop('sources', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hjmZ2liLghr"
      },
      "source": [
        "columns_to_remove = ['outCitations', 's2Url', 'pdfUrls', 'journalVolume', 'venue', 'journalName', 'doi', 'doiUrl', 's2PdfUrl', 'pmid', 'magId', 'entities']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HWrg-CpLikD"
      },
      "source": [
        "article_info.drop(columns_to_remove, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKPraqA0Lrij"
      },
      "source": [
        "article_info.journalName.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQChrDHWLs-k"
      },
      "source": [
        "article_info.venue.value_counts()[article_info.venue.value_counts() > 20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfTtY4j4LxK5"
      },
      "source": [
        "article_info.journalName.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62eH8EvcLyw8"
      },
      "source": [
        "# features for one-hot encoding\n",
        "# venue\n",
        "# journalName\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "enc = LabelEncoder()\n",
        "enc.fit(article_info.journalName.value_counts().index.values)\n",
        "article_info.journalName = enc.transform(article_info.journalName.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o5HDrqJL0Rs"
      },
      "source": [
        "enc = LabelEncoder()\n",
        "enc.fit(article_info.venue.value_counts().index.values)\n",
        "article_info.venue = enc.transform(article_info.venue.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQIpU-MyL2Z8"
      },
      "source": [
        "article_info.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyhD6TCKL33j"
      },
      "source": [
        "article_info.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOxstQwPL5z1"
      },
      "source": [
        "article_info.title.apply(lambda row: len(row))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmyYst3PL9EI"
      },
      "source": [
        "%%time\n",
        "\n",
        "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_process=2)]\n",
        "\n",
        "# print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrq80ghsL_Jb"
      },
      "source": [
        "data_clean = pd.DataFrame({'clean': txt})\n",
        "data_clean = data_clean.dropna().drop_duplicates()\n",
        "data_clean.shape"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}